"""
answer_generator.py

Generates an LLM-based answer from retrieved text chunks.
"""

from __future__ import annotations

import logging
import os
from typing import List

from openai import OpenAI


def _compose_context(results: List[dict]) -> str:
    """Format retrieved text chunks into a single context block for LLM input."""
    snippets = []
    # Start index at 1 instead of 0
    for idx, result in enumerate(results, 1):
        # Each chunk is labeled with its snippet number and PubMed ID (PMID) to help the
        # language model identify and cite relevant evidence during answer generation.
        snippets.append(f"Snippet {idx} [PMID {result['pmid']}]\n{result['chunk_text']}")
    # Blank lines are for readability.
    return "\n\n".join(snippets)


def _call_llm(client: OpenAI, model_name: str, system_prompt: str, user_prompt: str) -> str | None:
    """Send a question and context to an OpenAI language model and return the generated answer text."""
    try:
        # A chat completion request for older SDKs that only expose chat.completions
        # (newer SDK uses attribute called responses).
        # The `system` message sets rules or behavior.
        # The `user` message contains the actual question and any context.
        # Returns a structured JSON response containing the generated text plus metadata.
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
        )
        # `.output[0]` is the first response generated by the model.
        # `.content[0].text` is the actual text of the modelâ€™s reply.
        return response.choices[0].message.content.strip()
    except Exception as e:
        logging.exception("OpenAI ChatCompletion failed: %s", e)
        return None


def generate_answer(
    query_text: str,
    ordered_results: List[dict],
    model_name: str,
) -> str | None:
    """Generate a natural language answer from retrieved PubMed chunks using an OpenAI LLM."""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logging.error("OPENAI_API_KEY not set; cannot generate answer.")
        return None
    if not ordered_results:
        logging.warning("No retrieval results available for answer generation.")
        return None

    # Creates a new OpenAI API client instance
    client = OpenAI(api_key=api_key)
    # Combine all retrieved text chunks into a formatted string with labeled snippets and PMIDs.
    context_block = _compose_context(ordered_results)
    system_prompt = (
        "You are a helpful medical research assistant."
        " Given the provided context, answer the user's question concisely."
        " Cite supporting evidence inline using [PMID #######]."
        " If the answer is not present in the context, respond with that."
    )
    # Format the message for the model.
    user_prompt = f"Question: {query_text}\n\nContext:\n{context_block}"
    return _call_llm(client, model_name, system_prompt, user_prompt)
